{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import everything you need (e.g., model, dataset, etc.)\n",
    "# 2. Use Random Search to find the best hyperparams(e.g., batch_size, augmentation, optimizer, loss_fn, etc.)\n",
    "# 3. Report the best train configuration\n",
    "# 4. Save the best values (for hyperparams)\n",
    "# 5. Use the found values in \"train.ipynb\" to train the model with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tekboart/.local/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# reload modules\n",
    "from importlib import reload\n",
    "\n",
    "# load pretrained segmentation models (written in pytorch)\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision\n",
    "from torchviz import make_dot\n",
    "import torchvision.transforms.v2 as TF\n",
    "\n",
    "# torchmetrics\n",
    "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
    "\n",
    "# Ray Tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "# Serialize/Deserialize Json files\n",
    "import json\n",
    "\n",
    "# Data Augmentation\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# get data/time with desired format\n",
    "from datetime import datetime\n",
    "\n",
    "time_format = \"%Y.%m.%d@%H-%M-%S\"\n",
    "\n",
    "# work with images\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# slice Iterables and turn to GEN\n",
    "from itertools import islice\n",
    "\n",
    "# keep numpy use to a min\n",
    "# as we store our torch.Tensors to GPU Vram but numpy in RAM (it only supports CPU)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# to have a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To use pretrained segmentation models (implement in PyTorch)\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "# uncomment if wan't to globally aloow sns to handle plot style\n",
    "# it adds unwanted style (i.e., grid) to .imshow()\n",
    "# so better to use it a context manager to style only what I want\n",
    "# >>> with sns.axes_style('darkgrid'):\n",
    "# >>>     plt.imshow(...)\n",
    "# sns.set_theme(\n",
    "#     context=\"notebook\",\n",
    "#     style=\"darkgrid\",\n",
    "#     palette=\"deep\",\n",
    "#     font=\"sans-serif\",\n",
    "#     font_scale=1,\n",
    "#     color_codes=True,\n",
    "#     rc={'axes.grid': False},\n",
    "# )\n",
    "\n",
    "# OS/File/Path management\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Misc\n",
    "from functools import partial\n",
    "\n",
    "# load my custom Classes/Functions/etc.\n",
    "from utils.training import fit_fn\n",
    "from utils.dataset import get_loaders, SegmentaionDataset\n",
    "from utils.models.unet import UnetScratch\n",
    "from utils.visualization import (\n",
    "    image_mask_plot,\n",
    "    ImageAntiStandardize,\n",
    "    plot_metrics\n",
    ")\n",
    "from utils.metrics import (\n",
    "    AccuracyBinarySegment,\n",
    "    DiceBinarySegment,\n",
    "    JaccardBinarySegment,\n",
    "    PrecisionBinarySegment,\n",
    "    RecallBinarySegment,\n",
    "    F1BinarySegment,\n",
    "    DiceBCELoss,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[CUDA_VISIBLE_DEVICES]= \"1,3 ray start --head --num-gpus=2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove sources of non-determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8b6749ac70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use to seed the RNG for all devices (both CPU and CUDA).\n",
    "torch.manual_seed(0)\n",
    "# for custom operators, you might need to set python seed as well:\n",
    "random.seed(0)\n",
    "# If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with:\n",
    "np.random.seed(0)\n",
    "\n",
    "# configure PyTorch to use deterministic algorithms instead of nondeterministic ones \n",
    "# A CAVEAT: throws an error if an operation is known to be nondeterministic (and without a deterministic alternative).\n",
    "# that is why we used warn_only to avoid raising Error\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "# CUDA convolution determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# -- DataLoader\n",
    "# make it deterministic but allow it to random data order\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "DATA_LOADER_GEN = torch.Generator()\n",
    "DATA_LOADER_GEN.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Search Hyperparameters Space\n",
    "> that is, the hyperparameters and their correspondign values to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Config dict\n",
    "config = {\n",
    "    'scaling_config': ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    'lr': tune.loguniform(1e-4, 1e-2),\n",
    "    'scheduler_factor': tune.choice([.5, .3, .1]),\n",
    "    'batch_size': tune.choice([4, 8, 16, 32]),\n",
    "    'image_size': tune.choice([352, 256, 160]),  # sizes must be divisible by 32 \n",
    "    'pretrained_arch': tune.choice([smp.UnetPlusPlus, smp.FPN, smp.DeepLabV3Plus]),\n",
    "    #TODO: Maybe remove ResNet152 as it's both (1) comp expensive and (2) can overfit very easily (considering its #params)\n",
    "    'pretrained_encoder': tune.choice(['mobilenet_v2', 'timm-mobilenetv3_large_100', 'timm-efficientnet-b8', 'resnet152']),\n",
    "    'loss_fn': tune.choice([nn.BCEWithLogitsLoss(), DiceBCELoss(from_logits=True)]),\n",
    "    'train_augmentation': tune.choice([\n",
    "        #TODO: As my last model's val & test metrics were very different, then maybe I need stronger augmentations for the model to generalize better.\n",
    "        # We didn't included the resize/Flip/etc. that we know would cause no problem. so:\n",
    "        # Just use this A.Compose([*hyper_config['train_augmentation'], A.HorizontalFlip(...), A.Resize(..), etc.])\n",
    "        [\n",
    "            A.Rotate(limit=5, p=0.5),  # Use only when the img_height==img_width\n",
    "            A.RandomRotate90(p=0.5),  # Use only when the img_height==img_width\n",
    "            A.Transpose(p=0.5),  # Use only when the img_height==img_width\n",
    "            # A.CenterCrop(\n",
    "                #TODO: Remember to add vars image_height & image_width in the train func\n",
    "                # int(0.9 * image_height),\n",
    "                # int(0.9 * image_width),\n",
    "                # p=0.3,\n",
    "            # ),\n",
    "            A.ColorJitter(\n",
    "                brightness=0.3,\n",
    "                contrast=0.05,\n",
    "                saturation=0.1,\n",
    "                hue=0.05,\n",
    "                p=0.8,\n",
    "                # always_apply=True,\n",
    "            ),\n",
    "            A.ImageCompression(quality_lower=80, quality_upper=100, p=0.3),\n",
    "            A.RGBShift(\n",
    "                r_shift_limit=5,\n",
    "                g_shift_limit=5,\n",
    "                b_shift_limit=2,\n",
    "                p=0.8,\n",
    "                # always_apply=True,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            A.Rotate(limit=5, p=0.5),  # Use only when the img_height==img_width\n",
    "            A.RandomRotate90(p=0.5),  # Use only when the img_height==img_width\n",
    "            A.Transpose(p=0.5),  # Use only when the img_height==img_width\n",
    "            A.ColorJitter(\n",
    "                brightness=0.3,\n",
    "                contrast=0.05,\n",
    "                saturation=0.1,\n",
    "                hue=0.05,\n",
    "                p=0.8,\n",
    "                # always_apply=True,\n",
    "            ),\n",
    "            A.RGBShift(\n",
    "                r_shift_limit=5,\n",
    "                g_shift_limit=5,\n",
    "                b_shift_limit=2,\n",
    "                p=0.8,\n",
    "                # always_apply=True,\n",
    "            ),\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create The Model Func\n",
    "> Everything needed for trainin a model needs to be within its scope (should not use any var from outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model_creator_for_hyperserach to create + Compile a model (to be ready to be trained)\n",
    "# TODO: Can I make it reusable for other models as well?\n",
    "# A: I don't think\n",
    "def model_creator(config):\n",
    "    \"\"\"\n",
    "    instantiate + Compile + train (aka .fit()) a network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hyper_config: dict\n",
    "        Includes all the hyperparmeters needed to be tuned.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device used for calculation (CPU\\Cuda):\", device)\n",
    "\n",
    "    # Load all the needed variables (Ray Tune asks all the used variables to be inside this func scope)\n",
    "    hyper_params = {\n",
    "        \"device\": str(device),\n",
    "        \"lr\": 1e-3,\n",
    "        \"lr_finetune\": 1e-4,  # for transfer learning (phase 2) (= lr / 1000)\n",
    "        \"epochs\": 10,\n",
    "        \"epochs_finetune\": 10,  # for transfer learning (phase 2)\n",
    "        \"batch_size\": 16,\n",
    "        \"num_workers\": 2,\n",
    "        # Use Height == Width to use 90-rotations/transpose in data aug\n",
    "        \"image_height\": 352,\n",
    "        \"image_width\": 352,\n",
    "        \"input_channels\": 3,\n",
    "        \"num_classes\": 1,\n",
    "        \"data_format\": \"channels_first\",\n",
    "        \"pin_mem\": True,\n",
    "        \"scheduler_step\": 5,\n",
    "        \"scheduler_factor\": 0.5,\n",
    "        \"scheduler_factor_finetune\": 0.5,  # for transfer learning (phase 2)\n",
    "        \"train_img_dir\": os.path.join(\"data\", \"traincrop\", \"img\"),\n",
    "        \"train_mask_dir\": os.path.join(\"data\", \"traincrop\", \"mask\"),\n",
    "        \"val_img_dir\": os.path.join(\"data\", \"valcrop\", \"img\"),\n",
    "        \"val_mask_dir\": os.path.join(\"data\", \"valcrop\", \"mask\"),\n",
    "        \"test_img_dir\": os.path.join(\"data\", \"testcrop\", \"img\"),\n",
    "        \"test_mask_dir\": os.path.join(\"data\", \"testcrop\", \"mask\"),\n",
    "        \"pretrained_model_encoder\": \"timm-mobilenetv3_large_100\",\n",
    "    }\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    DATA_LOADER_GEN = torch.Generator()\n",
    "    DATA_LOADER_GEN.manual_seed(0)\n",
    "\n",
    "    # Load a Pretrained Model\n",
    "    # TODO: Load the preprocess_fn before, so no need to create it for each run\n",
    "    model_arch = config[\"pretrained_arch\"]\n",
    "\n",
    "    # create the model\n",
    "    model = model_arch(\n",
    "        # choose encoder\n",
    "        encoder_name=config[\"pretrained_encoder\"],\n",
    "        # use `imagenet` pre-trained weights for encoder initialization\n",
    "        encoder_weights=\"imagenet\",\n",
    "        # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        in_channels=hyper_params[\"input_channels\"],\n",
    "        # model output channels (number of classes in your dataset)\n",
    "        classes=hyper_params[\"num_classes\"],\n",
    "    )\n",
    "\n",
    "    preprocess_input = get_preprocessing_fn(\n",
    "        config[\"pretrained_encoder\"], pretrained=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    # Create Data Augmentation\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            *config[\"train_augmentation\"],\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Resize(height=config[\"image_size\"], width=config[\"image_size\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # we don't want TTA, just some resize, normalization, etc.\n",
    "    val_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=config[\"image_size\"], width=config[\"image_size\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create the Datasets\n",
    "    train_ds = SegmentaionDataset(\n",
    "        image_dir=hyper_params[\"train_img_dir\"],\n",
    "        mask_dir=hyper_params[\"train_mask_dir\"],\n",
    "        transform=train_transform,\n",
    "        preprocess_fn=preprocess_input,\n",
    "        mask_suffix=\"\",\n",
    "        # subset=[0, 10],\n",
    "    )\n",
    "    val_ds = SegmentaionDataset(\n",
    "        image_dir=hyper_params[\"val_img_dir\"],\n",
    "        mask_dir=hyper_params[\"val_mask_dir\"],\n",
    "        transform=val_transform,\n",
    "        preprocess_fn=preprocess_input,\n",
    "        mask_suffix=\"\",\n",
    "        # subset=[0, 5],\n",
    "    )\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=hyper_params[\"num_workers\"],\n",
    "        pin_memory=hyper_params[\"pin_mem\"],\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=DATA_LOADER_GEN,\n",
    "    )\n",
    "\n",
    "    # Define Metrics\n",
    "    metrics = {\n",
    "        \"jaccard (IOU)\": JaccardBinarySegment(from_logits=True),\n",
    "        \"recall\": RecallBinarySegment(from_logits=True),\n",
    "        \"precision\": PrecisionBinarySegment(from_logits=True),\n",
    "        \"dice (F1-Score)\": DiceBinarySegment(from_logits=True),\n",
    "    }\n",
    "\n",
    "    # Define Loss_fn, Optimizer, and Scheduler\n",
    "    loss_fn = config[\"loss_fn\"]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=config[\"scheduler_factor\"],\n",
    "        # factor=hyper_params[\"scheduler_factor\"],\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        threshold=1e-3,\n",
    "    )\n",
    "\n",
    "    # train the model (and get the history)\n",
    "    # TODO: must change the fit_fn to use session.report(...)\n",
    "    # TODO: use verbose=False to make the output of ray tune clean\n",
    "    history = fit_fn(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        scheduler,\n",
    "        metrics=metrics,\n",
    "        val_loader=val_loader,\n",
    "        epochs=hyper_params[\"epochs\"],\n",
    "        device=device,\n",
    "        ray_tune=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partial func of the model_creator (using functools.partial)\n",
    "# as we only change the hyperparams of interest for each run of Ray Tune (i.e., hyperparam_tuner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tuner Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tuner(\n",
    "    config: dict,\n",
    "    num_samples: int = 10,\n",
    "    max_num_epochs: int = 10,\n",
    "    tune_metric: str = \"val_loss\",\n",
    "    tune_metric_mode: str = \"min\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a model and a set of hyperparameters,\n",
    "    tries to find the best combination and return the best performing model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hyper_config: dict\n",
    "    max_num_epochs: int\n",
    "    num_samples: int\n",
    "        Define the total number of samples/combinatin to use to train the model.\n",
    "        In other words, the number of models to train for finding the best hyperparameters.\n",
    "    tune_metric: str\n",
    "        The name of the metric (e.g., val_loss) by which the performance of mdoels are assessed.\n",
    "    tune_metric_mode: str\n",
    "        Either \"min\" (default) or \"max\", depending on the chosen tune_metric. Specifies whether \"min\" values are desired, or vice versa.\n",
    "    gpus_per_trial: int\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # use ASHAScheduler to stop training early on if the model's plight is doomed\n",
    "    scheduler_ray = ASHAScheduler(\n",
    "        # metric=\"loss\",  # this is specified in the Tuner's tune.TuneConifg, so only one must be present.\n",
    "        # mode=\"min\",  # this is specified in the Tuner's tune.TuneConifg, so only one must be present.\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    # method 1 (less control): from Pytorch Tutorials\n",
    "    # results = tune.run(\n",
    "    #     model_creator,\n",
    "    #     resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "    #     config=hyper_config,\n",
    "    #     num_samples=num_samples,\n",
    "    #     scheduler=scheduler_ray,\n",
    "    # )\n",
    "\n",
    "    # method 2 (more control): from Ray Tune official documentaions\n",
    "    tuner = tune.Tuner(\n",
    "        model_creator,\n",
    "        param_space=config,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=num_samples,\n",
    "            metric=tune_metric,\n",
    "            mode=tune_metric_mode,\n",
    "            search_alg=OptunaSearch(),  # Optuna Works better than Random Search (BOn Pytorch official YouTube)\n",
    "            scheduler=scheduler_ray,\n",
    "            chdir_to_trial_dir=False,  # if True (default) the Ray tune will change the current dir, thus no relative path (e.g., path to our data) doesn't work.\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_trial = results.get_best_trial(tune_metric, tune_metric_mode, \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "\n",
    "    metrics = [\"loss\", \"jaccard (IOU)\", \"dice (F1-Score)\", \"recall\", \"precision\"]\n",
    "    for metric in metrics:\n",
    "        print(f\"Best trial final {metric}: {best_trial.last_result[metric]}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 11:43:59,851\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "2023-07-13 11:44:01,061\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/ray/tune/search/optuna/optuna_search.py:664: FutureWarning: LogUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n",
      "  return ot.distributions.LogUniformDistribution(\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ('__ref_ph', 'c70cbae7') which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ('__ref_ph', 'd08b895f') which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ('__ref_ph', 'c9fd45ce') which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ('__ref_ph', 'f214218d') which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ('__ref_ph', '881911f9') which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [('__ref_ph', '0a114c4f'), ('__ref_ph', '1ae38e74'), ('__ref_ph', 'e52c4284'), ('__ref_ph', 'd1d84746'), ('__ref_ph', 'c2045ddd'), ('__ref_ph', '3de8a409')] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [('__ref_ph', '97fc5fbf'), ('__ref_ph', '068cc855'), ('__ref_ph', '250c953c'), ('__ref_ph', '1a0165b8'), ('__ref_ph', '9caf1824')] which is of type list.\n",
      "  warnings.warn(message)\n",
      "[I 2023-07-13 11:44:01,077] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-13 11:44:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:55.64        </td></tr>\n",
       "<tr><td>Memory:      </td><td>29.1/31.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 7.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  image_size</th><th>loss_fn            </th><th style=\"text-align: right;\">         lr</th><th>pretrained_arch     </th><th>pretrained_encoder  </th><th style=\"text-align: right;\">  scheduler_factor</th><th>train_augmentation  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>model_creator_199973df</td><td>RUNNING </td><td>192.168.1.103:249063</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         256</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.00952865 </td><td>&lt;class &#x27;segment_0de0</td><td>resnet152           </td><td style=\"text-align: right;\">               0.3</td><td>[Rotate(always__0300</td></tr>\n",
       "<tr><td>model_creator_3e75b2da</td><td>RUNNING </td><td>192.168.1.103:249122</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">         352</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.000625776</td><td>&lt;class &#x27;segment_0de0</td><td>timm-mobilenetv_3460</td><td style=\"text-align: right;\">               0.5</td><td>[Rotate(always__23c0</td></tr>\n",
       "<tr><td>model_creator_b854f941</td><td>RUNNING </td><td>192.168.1.103:249269</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         256</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.000635212</td><td>&lt;class &#x27;segment_a3a0</td><td>resnet152           </td><td style=\"text-align: right;\">               0.3</td><td>[Rotate(always__5040</td></tr>\n",
       "<tr><td>model_creator_5bf91585</td><td>RUNNING </td><td>192.168.1.103:249438</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         160</td><td>BCEWithLogitsLoss()</td><td style=\"text-align: right;\">0.00284378 </td><td>&lt;class &#x27;segment_a3a0</td><td>resnet152           </td><td style=\"text-align: right;\">               0.1</td><td>[Rotate(always__1b00</td></tr>\n",
       "<tr><td>model_creator_3e393ab4</td><td>RUNNING </td><td>192.168.1.103:249605</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">         160</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.000265173</td><td>&lt;class &#x27;segment_6430</td><td>resnet152           </td><td style=\"text-align: right;\">               0.5</td><td>[Rotate(always__7400</td></tr>\n",
       "<tr><td>model_creator_594054e9</td><td>RUNNING </td><td>192.168.1.103:249797</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">         160</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.00232954 </td><td>&lt;class &#x27;segment_6430</td><td>timm-efficientnet-b8</td><td style=\"text-align: right;\">               0.5</td><td>[Rotate(always__6880</td></tr>\n",
       "<tr><td>model_creator_9a84860a</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">         352</td><td>DiceBCELoss()      </td><td style=\"text-align: right;\">0.00123429 </td><td>&lt;class &#x27;segment_6430</td><td>timm-efficientnet-b8</td><td style=\"text-align: right;\">               0.5</td><td>[Rotate(always__a240</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tekboart/.local/lib/python3.11/site-packages/optuna/distributions.py:781: FutureWarning: LogUniformDistribution(high=0.01, low=0.0001) is deprecated and internally converted to FloatDistribution(high=0.01, log=True, low=0.0001, step=None). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m 2023-07-13 11:44:07.645562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m 2023-07-13 11:44:08.578272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m /home/tekboart/.local/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m   warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "Epoch 1:   0%|          | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249063)\u001b[0m ---------------------------------- epoch 1/10 ---------------------------------\n",
      "\u001b[2m\u001b[36m(model_creator pid=249122)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 1/60 [00:02<01:59,  2.03s/it]\n",
      "Epoch 1:   5%|▌         | 3/60 [00:03<01:10,  1.23s/it]\n",
      "Epoch 1:   5%|▌         | 3/60 [00:03<01:10,  1.23s/it]\n",
      "Epoch 1:   5%|▌         | 3/60 [00:03<01:10,  1.23s/it]\n",
      "Epoch 1:   1%|          | 2/237 [00:00<00:58,  4.04it/s]\n",
      "Epoch 1:   1%|          | 2/237 [00:00<00:58,  4.04it/s]\n",
      "Epoch 1:   1%|▏         | 3/237 [00:00<01:06,  3.52it/s]\n",
      "Epoch 1:   1%|▏         | 3/237 [00:00<01:06,  3.52it/s]\n",
      "Epoch 1:   1%|▏         | 3/237 [00:00<01:06,  3.52it/s]\n",
      "Epoch 1:   5%|▌         | 13/237 [00:02<00:50,  4.48it/s]\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n",
      "\u001b[2m\u001b[36m(model_creator pid=249122)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m 2023-07-13 11:44:21.344605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Epoch 1:  16%|█▋        | 39/237 [00:08<00:43,  4.53it/s]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m 2023-07-13 11:44:22.431635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m /home/tekboart/.local/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m   warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "Epoch 1:   0%|          | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249269)\u001b[0m ---------------------------------- epoch 1/10 ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 61/237 [00:13<00:42,  4.19it/s]\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m 2023-07-13 11:44:30.697978: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Epoch 1:  34%|███▍      | 81/237 [00:18<00:41,  3.73it/s]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m 2023-07-13 11:44:32.174641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m /home/tekboart/.local/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m   warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "Epoch 1:   0%|          | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249438)\u001b[0m ---------------------------------- epoch 1/10 ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 99/237 [00:23<00:39,  3.49it/s]\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m 2023-07-13 11:44:41.346088: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Epoch 1:  49%|████▊     | 115/237 [00:28<00:39,  3.12it/s]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m 2023-07-13 11:44:43.133297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m /home/tekboart/.local/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m   warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "Epoch 1:   0%|          | 0/474 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249605)\u001b[0m ---------------------------------- epoch 1/10 ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▎         | 17/474 [00:02<01:06,  6.85it/s]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 1:  42%|████▏     | 25/60 [00:28<00:47,  1.36s/it]\u001b[32m [repeated 27x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(model_creator pid=249797)\u001b[0m Device used for calculation (CPU\\Cuda): cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 19/60 [00:24<01:09,  1.70s/it]\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #TODO: Force Ray Tune to use GPU (use >2 values if you have multiple ones)\n",
    "    # ray.init(num_cpus=8, num_gpus=1)\n",
    "\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    hyperparam_tuner(config=config, num_samples=10, max_num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Obtain a trial dataframe from all run trials of this `tune.run` call.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dfs \u001b[39m=\u001b[39m {result\u001b[39m.\u001b[39mlog_dir: result\u001b[39m.\u001b[39mmetrics_dataframe \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results}\n\u001b[1;32m      4\u001b[0m \u001b[39m# Plot by epoch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# This plots everything on the same plot\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Obtain a trial dataframe from all run trials of this `tune.run` call.\n",
    "dfs = {result.log_dir: result.metrics_dataframe for result in results}\n",
    "\n",
    "# Plot by epoch\n",
    "ax = None  # This plots everything on the same plot\n",
    "for d in dfs.values():\n",
    "    ax = d.val_loss.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel(\"val_loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tekboart/.local/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/tekboart/.local/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "2023-07-06 00:20:28.859098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 00:20:29.600914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data Augmentation\n",
    "import albumentations as A\n",
    "# import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# torchvision\n",
    "import torchvision.transforms.v2 as TF\n",
    "from torchviz import make_dot\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# to have a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To use pretrained segmentation models (implement in PyTorch)\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# OS/File/Path management\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# load my custom Classes/Functions/etc.\n",
    "from utils.models.unet import UnetScratch\n",
    "from utils.dataset import SegmentaionDataset\n",
    "from utils.inferencing import inference_segmentation, img_to_inference_tensor\n",
    "from utils.visualization import torch_tensor_for_plt, plot_segmentation_inference\n",
    "\n",
    "\n",
    "# from utils import (\n",
    "#     load_checkpoint,\n",
    "#     save_checkpoint,\n",
    "#     get_loaders,\n",
    "#     check_accuracy,\n",
    "#     save_pred_as_imgs\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up GPU use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ver: 2.0.1+cu118\n",
      "Can I use GPU? True\n",
      "Device used for calculation (CPU\\Cuda): cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch ver:', torch.__version__)\n",
    "\n",
    "# force pytorch to use GPU\n",
    "# use \"model.to(device)\" later on to force a model use Cuda GPU\n",
    "print('Can I use GPU?', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device used for calculation (CPU\\Cuda):', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device\n",
    "lr = 1e-4\n",
    "batch_size = 2\n",
    "epochs = 5\n",
    "num_workers = 2\n",
    "image_height = 384\n",
    "image_width = 640\n",
    "num_classes = 1\n",
    "pin_mem = True\n",
    "load_model = False\n",
    "load_model_path = os.path.join('models')\n",
    "\n",
    "# define path(s)\n",
    "train_img_dir = os.path.join(\"data\", \"traincrop\", \"img\")\n",
    "train_mask_dir = os.path.join(\"data\", \"traincrop\", \"mask\")\n",
    "val_img_dir = os.path.join(\"data\", \"valcrop\", \"img\")\n",
    "val_mask_dir = os.path.join(\"data\", \"valcrop\", \"mask\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import get_loaders\n",
    "\n",
    "\n",
    "# define transformers (resize, rescale, augmentation, etc.)\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=image_height, width=image_width),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        # this is for z-score\n",
    "        # for pretrained models on, for instance, imagenet need other values for mean and std.\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0, 0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0\n",
    "        ),\n",
    "        # ToTensorV2(),\n",
    "    ],\n",
    "    # TODO: this options seems risky\n",
    "    # is_check_shapes=False,\n",
    ")\n",
    "\n",
    "# we don't want TTA, just some resize, normalization, etc.\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=image_height, width=image_width),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0, 0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0\n",
    "        ),\n",
    "        # ToTensorV2(),\n",
    "    ],\n",
    "    # TODO: this options seems risky\n",
    "    # is_check_shapes=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Create Datatset by data loaders\n",
    "train_loader, val_loader = get_loaders(\n",
    "    train_img_dir,\n",
    "    train_mask_dir,\n",
    "    val_img_dir,\n",
    "    val_mask_dir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers,\n",
    "    pin_mem,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train images: 6\n",
      "# val images: 8\n"
     ]
    }
   ],
   "source": [
    "print('# train images:', sum(len(batch) for batch in train_loader))\n",
    "print('# val images:', sum(len(batch) for batch in val_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import a Pretrained Segmentaion model (e.g., UNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the downloaded model are located in \"~/.cache/torch/hub/checkpoints/\"\n",
    "# backbone_model_name = 'resnet152'\n",
    "backbone_model_name = 'mobilenet_v2'\n",
    "\n",
    "# Segmentation model is just a PyTorch nn.Module\n",
    "# model = smp.FPN(\n",
    "model = smp.Unet(\n",
    "    encoder_name=backbone_model_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7 \n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "# load preprocessing func of the loaded model, so our data goes through the same transformation\n",
    "# hint 1: it needs the input img to be in channels_last format\n",
    "# hint 2: it outputs the image in channels_last format! (e.g., toch.Size[640, 360, 3])\n",
    "preprocess_input = get_preprocessing_fn(backbone_model_name, pretrained='imagenet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# the attribs of the model\n",
    "# vars(model)\n",
    "\n",
    "\n",
    "# plot the model arch\n",
    "# create a dumy channels_first img (m, C, H, W)\n",
    "x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\n",
    "yhat = model(x)\n",
    "print(yhat.shape)\n",
    "\n",
    "# save the architecture of the model as an image\n",
    "# make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"./outputs/model_architecture/Unet_pretrained\", format=\"png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model with an img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Segmentation Model (i.e., UNET), written from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetScratch(in_channels=3, num_classes=num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch 0's metric(s) (training) -----------------------\n",
      "accuracy    0.53\n",
      "dice        1.90\n",
      "----------------------- epoch 0's metric(s) (validation) ----------------------\n",
      "val_accuracy:  0.05\n",
      "val_dice:   0.09\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch 1's metric(s) (training) -----------------------\n",
      "accuracy    0.72\n",
      "dice       -10.01\n",
      "----------------------- epoch 1's metric(s) (validation) ----------------------\n",
      "val_accuracy:  0.05\n",
      "val_dice:   0.09\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch 2's metric(s) (training) -----------------------\n",
      "accuracy    0.86\n",
      "dice       -2.24\n",
      "----------------------- epoch 2's metric(s) (validation) ----------------------\n",
      "val_accuracy:  0.05\n",
      "val_dice:   0.09\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch 3's metric(s) (training) -----------------------\n",
      "accuracy    0.93\n",
      "dice       -1.28\n",
      "----------------------- epoch 3's metric(s) (validation) ----------------------\n",
      "val_accuracy:  0.24\n",
      "val_dice:   0.08\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch 4's metric(s) (training) -----------------------\n",
      "accuracy    0.97\n",
      "dice       -0.85\n",
      "----------------------- epoch 4's metric(s) (validation) ----------------------\n",
      "val_accuracy:  0.93\n",
      "val_dice:   0.01\n",
      "-------------------------------------------------------------------------------\n",
      "----------------------- Saving Checkpoint (In progress) -----------------------\n",
      "\n",
      "Checkpoint was saved as: ./models/2023.07.06@00-20-53-model_checkpoint.pth.tar\n",
      "\n",
      "--------------------------- Saving Checkpoint (Done) --------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.training import train_model\n",
    "from utils.metrics import accuracy_segment, dice_segment\n",
    "\n",
    "metrics = (\"accuracy\", \"dice\")\n",
    "# a dict to map metrics' name to correspoing fn\n",
    "metrics_fn = {\n",
    "    \"dice\": dice_segment,\n",
    "    \"accuracy\": accuracy_segment,\n",
    "}\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    device=device,\n",
    "    metrics=metrics,\n",
    "    metrics_fn=metrics_fn,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_addr = os.path.join('images', '') + '20160222_081011_1_721.jpg'\n",
    "mask_addr = os.path.join('images', '') + '20160222_081011_1_721_mask.png'\n",
    "\n",
    "# inference_semantic_seg(img_addr, model=model, thresh=0.5, )\n",
    "\n",
    "img = Image.open(img_addr)\n",
    "mask = Image.open(mask_addr)\n",
    "\n",
    "# normalize [-1, 1] the PIL image as a torch.Tensor\n",
    "img_tensor = (TF.functional.pil_to_tensor(img) / 127.5 - 1)\n",
    "\n",
    "yhat = model(img_tensor.unsqueeze(dim=0))\n",
    "yhat_c_last = yhat[0].moveaxis(0, -1).detach().numpy()\n",
    "thresh = 0.5\n",
    "mask_batch = np.where(yhat_c_last >= thresh, 255, 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Image', fontsize=16)\n",
    "axes[1].imshow(mask_batch, cmap='gray')\n",
    "axes[1].set_title('Mask (predicted)', fontsize=16)\n",
    "axes[2].imshow(mask, cmap='gray')\n",
    "axes[2].set_title('Mask (target)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m test_mask_batch \u001b[39m=\u001b[39m img_to_inference_tensor(mask_addr, size\u001b[39m=\u001b[39m(image_height, image_width))\n\u001b[1;32m      8\u001b[0m \u001b[39m# make an inference\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m yhat_mask \u001b[39m=\u001b[39m inference_segmentation(test_img_batch, model\u001b[39m=\u001b[39;49mmodel, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# make torch.Tensors ready for pyplot\u001b[39;00m\n\u001b[1;32m     12\u001b[0m test_img_batch \u001b[39m=\u001b[39m torch_tensor_for_plt(test_img_batch)\n",
      "File \u001b[0;32m/run/media/tekboart/SP 2TB/CE - CS/Development/Exercise/0 Interview/HKUST/utils/inferencing.py:112\u001b[0m, in \u001b[0;36minference_segmentation\u001b[0;34m(img_batch, model, num_classes, thresh, data_format, normalize, preprocess_func)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(img_batch\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    110\u001b[0m     img_batch \u001b[39m=\u001b[39m img_batch\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m yhat \u001b[39m=\u001b[39m model(img_batch)\n\u001b[1;32m    114\u001b[0m \u001b[39m# construct the final mask (a gray img)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m num_classes \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/run/media/tekboart/SP 2TB/CE - CS/Development/Exercise/0 Interview/HKUST/utils/models/unet.py:216\u001b[0m, in \u001b[0;36mUnetScratch.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mThe forward pass for this Class\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    1. x: A channels_first Torch 4D-Tensor of shape (m, C, H, W)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m# do the encoder part\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m x, skp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder(x)\n\u001b[1;32m    218\u001b[0m \u001b[39m# do the bottleneck part\u001b[39;00m\n\u001b[1;32m    219\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbottle_neck(x)\n",
      "File \u001b[0;32m/run/media/tekboart/SP 2TB/CE - CS/Development/Exercise/0 Interview/HKUST/utils/models/unet.py:158\u001b[0m, in \u001b[0;36mUnetScratch._encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m# do the encoder part\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder:\n\u001b[0;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m step(x)\n\u001b[1;32m    159\u001b[0m     skip_connections\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    160\u001b[0m     \u001b[39m# we didn't add pool layers in the self.encoder as we wanted to store/cache the output of each step before applying the MaxPool\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/run/media/tekboart/SP 2TB/CE - CS/Development/Exercise/0 Interview/HKUST/utils/models/unet.py:73\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    The forward pass of this NN (nn.Module)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m    args:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m        1. x: A channels_first Torch 4D-Tensor of shape (m, C, H, W)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "img_addr = os.path.join('images', '') + '20160222_081011_1_721.jpg'\n",
    "mask_addr = os.path.join('images', '') + '20160222_081011_1_721_mask.png'\n",
    "\n",
    "# only for one image, if multiple use utils.dataset\n",
    "test_img_batch = img_to_inference_tensor(img_addr, size=(image_height, image_width))\n",
    "test_mask_batch = img_to_inference_tensor(mask_addr, size=(image_height, image_width))\n",
    "\n",
    "# make an inference\n",
    "yhat_mask = inference_segmentation(test_img_batch, model=model, normalize=True)\n",
    "\n",
    "# make torch.Tensors ready for pyplot\n",
    "test_img_batch = torch_tensor_for_plt(test_img_batch)\n",
    "test_mask_batch = torch_tensor_for_plt(test_mask_batch)\n",
    "yhat_mask = torch_tensor_for_plt(yhat_mask)\n",
    "\n",
    "# plot the inference\n",
    "plot_segmentation_inference(test_img_batch, test_mask_batch, yhat_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
